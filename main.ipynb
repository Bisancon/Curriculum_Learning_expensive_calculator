{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: [1, 13, 15, 5, 11, 14, 8, 12, 11, 2]\n",
      "Обратное преобразование: <SOS>35-14=21<EOS>\n",
      "\n",
      "Токены: [1, 11, 17, 6, 14, 15, 16, 8, 17, 17, 15, 12, 2]\n",
      "Обратное преобразование: <SOS>17*456=7752<EOS>\n",
      "\n",
      "Токены: [1, 11, 10, 12, 15, 3, 12, 8, 11, 2]\n",
      "Обратное преобразование: <SOS>1025<UNK>2=1<EOS>\n",
      "Токены: [1, 3, 11, 10, 12, 15, 3, 12, 3, 4, 13, 8, 11, 2]\n",
      "Обратное преобразование: <SOS><UNK>1025<UNK>2<UNK>+3=1<EOS>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from DecoderTrans import DecoderTransformer\n",
    "from tokenizer import CurriculumTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "class MathDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, expressions, targets):\n",
    "        # expressions - это список/массив последовательностей, targets - это целевые значения\n",
    "        self.expressions = expressions\n",
    "        self.targets = targets\n",
    "        self.max_length = max(len(exp) for exp in expressions)  # Определяем максимальную длину\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expressions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        expression = self.expressions[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        # Преобразуем выражение в список, если оно не в нужном формате\n",
    "        if isinstance(expression, tuple):\n",
    "            expression = list(expression)\n",
    "        elif not isinstance(expression, list):\n",
    "            raise ValueError(f\"Expected list or tuple, got {type(expression)}\")\n",
    "\n",
    "        # Паддинг последовательности до максимальной длины\n",
    "        padded_expression = self.pad_sequences(expression, self.max_length)\n",
    "        \n",
    "        return padded_expression, target\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_sequences(seq, max_length):\n",
    "        # Проверка на правильный тип данных\n",
    "        if isinstance(seq, torch.Tensor):\n",
    "            seq = seq.tolist()  # Преобразуем в список, если это тензор\n",
    "        if not isinstance(seq, list):\n",
    "            raise ValueError(f\"Expected list or tensor, got {type(seq)}\")\n",
    "\n",
    "        padding_length = max_length - len(seq)\n",
    "        \n",
    "        # Добавляем паддинг (проверяем, что padding_length - это целое число)\n",
    "        if padding_length > 0:\n",
    "            padded_seq = F.pad(torch.tensor(seq, dtype=torch.long), (0, padding_length), value=0)\n",
    "        else:\n",
    "            padded_seq = torch.tensor(seq, dtype=torch.long)\n",
    "        \n",
    "        return padded_seq\n",
    "\n",
    "seq = [1, 2, 3, 4]\n",
    "max_length = 10\n",
    "\n",
    "padded_seq = MathDataset.pad_sequences(seq, max_length)\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    expressions, targets = zip(*batch)\n",
    "    max_length = max(len(exp) for exp in expressions)\n",
    "    expressions_padded = [MathDataset.pad_sequences(exp, max_length) for exp in expressions]\n",
    "    return torch.stack(expressions_padded), torch.tensor(targets)\n",
    "def generate_mask(tensor, pad_value=0):\n",
    "    return (tensor != pad_value).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tokenize(expression, tokenizer, vocab_size):\n",
    "    \"\"\"\n",
    "    Безопасная токенизация: если токен выходит за пределы словаря, он заменяется на unk_token.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(expression)  # Используем метод encode, а не tokenize\n",
    "    return [\n",
    "        token if token < vocab_size else tokenizer.vocab['<UNK>']  # Подставляем <UNK> для недопустимых индексов\n",
    "        for token in tokens\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные из CSV\n",
    "train_data = pd.read_csv('shuffled_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Предполагается, что у вас есть ваш токенизатор и спецификации для токенов\n",
    "tokenizer = CurriculumTokenizer()\n",
    "vocab = tokenizer.vocab  # Получаем словарь токенизатора\n",
    "vocab_size = len(vocab)  # Определяем размер словаря\n",
    "\n",
    "# Токенизация данных с использованием исправленной безопасной токенизации\n",
    "train_tokens = [safe_tokenize(expr, tokenizer, vocab_size) for expr in train_data['expression']]\n",
    "test_tokens = [safe_tokenize(expr, tokenizer, vocab_size) for expr in test_data['expression']]\n",
    "\n",
    "# Создаем датасеты\n",
    "train_dataset = MathDataset(train_tokens, train_data['result'].values)\n",
    "test_dataset = MathDataset(test_tokens, test_data['result'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DecoderTransformer(num_tokens=tokenizer.vocab_size, n_embd=128).to(device)  # Исправлено здесь\n",
    "criterion = nn.MSELoss()  # Поскольку мы предсказываем числа\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(input_sequence):\n",
    "    targets = input_sequence[1:].clone()  # Сдвигаем на 1\n",
    "    padding = input_sequence.new_zeros((1, input_sequence.size(1)))  # Добавляем паддинг\n",
    "    targets = torch.cat([padding, targets], dim=0)\n",
    "    \n",
    "    # Проверка индексов\n",
    "    assert torch.max(targets) < vocab_size, f\"Target index {torch.max(targets)} out of range!\"\n",
    "    return targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, criterion, optimizer, device, vocab_size):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:\n",
    "        expressions_tensor, token_ids = batch\n",
    "\n",
    "        # Проверка максимального и минимального индекса\n",
    "        print(f\"Max token index in batch: {token_ids.max()}\")\n",
    "        print(f\"Min token index in batch: {token_ids.min()}\")\n",
    "\n",
    "        # Проверка индексов, выходящих за пределы словаря\n",
    "        if token_ids.max() >= vocab_size or token_ids.min() < 0:\n",
    "            raise ValueError(f\"Token index out of range: max {token_ids.max()}, vocab_size {vocab_size}\")\n",
    "        \n",
    "        # Приведение токенов к допустимым индексам (если нужно)\n",
    "        token_ids = token_ids.clamp(0, vocab_size - 1)\n",
    "\n",
    "        expressions_tensor, token_ids = expressions_tensor.to(device), token_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Прямой проход (model принимает два аргумента)\n",
    "        outputs = model(expressions_tensor, token_ids)  # Передаем два аргумента\n",
    "        \n",
    "        # Потеря\n",
    "        loss = criterion(outputs.view(-1, vocab_size), token_ids.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for expressions, targets in data_loader:\n",
    "            expressions = expressions.long().to(device)  # Convert to LongTensor\n",
    "            targets = targets.to(device).float()  # Ensure targets are float32\n",
    "\n",
    "            # Проверка размерности перед передачей в модель\n",
    "            print(f'Input expressions tensor shape: {expressions.shape}')\n",
    "            print(f'Input targets tensor shape: {targets.shape}')\n",
    "            \n",
    "            # Выводим максимальные индексы\n",
    "            print(f'Max index in expressions_tensor: {expressions.max()}')\n",
    "            \n",
    "            outputs = model(expressions)\n",
    "\n",
    "            # Вычисление потери\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token index in batch: 8366\n",
      "Min token index in batch: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Token index out of range: max 8366, vocab_size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23360\\511402731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23360\\1171852813.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, criterion, optimizer, device, vocab_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Проверка индексов, выходящих за пределы словаря\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Token index out of range: max {token_ids.max()}, vocab_size {vocab_size}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Приведение токенов к допустимым индексам (если нужно)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Token index out of range: max 8366, vocab_size 19"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device, vocab_size)\n",
    "    test_loss = evaluate_model(model, test_loader, device)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация потерь\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
